# 9.6 AIæ€»ç»“æ–‡çŒ®è¦ç‚¹

## åœºæ™¯éœ€æ±‚

**é˜…è¯»æ–‡çŒ®çš„å›°å¢ƒ**:
```
ä¸‹è½½äº†50ç¯‡æ–‡çŒ®,æ¯ç¯‡10-20é¡µ
å®Œæ•´é˜…è¯»éœ€è¦: 50ç¯‡ Ã— 30åˆ†é’Ÿ = 25å°æ—¶ ğŸ˜«

å…³é”®é—®é¢˜:
- å“ªäº›æ–‡çŒ®æœ€ç›¸å…³?
- æ¯ç¯‡çš„æ ¸å¿ƒå‘ç°æ˜¯ä»€ä¹ˆ?
- æœ‰å“ªäº›å±€é™æ€§?
```

**AIè§£å†³æ–¹æ¡ˆ**:
```
AIå¿«é€Ÿé˜…è¯» â†’ 3åˆ†é’Ÿ/ç¯‡
æå–æ ¸å¿ƒè¦ç‚¹ â†’ ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦
æ—¶é—´èŠ‚çœ: 25å°æ—¶ â†’ 2.5å°æ—¶ (æ•ˆç‡æå‡10å€!)
```

## æ–¹æ³•1: ä½¿ç”¨ChatGPTæ€»ç»“

### åŸºç¡€Prompt

**å¤åˆ¶PDFæ–‡æœ¬,å‘ChatGPTæé—®**:
```
è¯·æ€»ç»“è¿™ç¯‡åŒ»å­¦æ–‡çŒ®,åŒ…æ‹¬:

1. ç ”ç©¶ç›®çš„
2. ç ”ç©¶æ–¹æ³•(æ ·æœ¬é‡ã€è®¾è®¡ç±»å‹)
3. ä¸»è¦å‘ç°(ç”¨æ•°æ®æ”¯æŒ)
4. ç»“è®ºå’Œä¸´åºŠæ„ä¹‰
5. å±€é™æ€§

è¦æ±‚:
- æ¯éƒ¨åˆ†3-5å¥è¯
- çªå‡ºå…³é”®æ•°æ®
- å®¢è§‚ä¸­ç«‹

[ç²˜è´´æ–‡çŒ®å…¨æ–‡æˆ–æ‘˜è¦]
```

### è¿›é˜¶Prompt:å¯¹æ¯”åˆ†æ

```
æˆ‘æœ‰3ç¯‡å…³äºé«˜è¡€å‹æ²»ç–—çš„æ–‡çŒ®,è¯·å¸®æˆ‘:

1. å¯¹æ¯”ç ”ç©¶è®¾è®¡å’Œæ ·æœ¬é‡
2. å¯¹æ¯”ä¸»è¦ç»“æœå’Œæœ‰æ•ˆæ€§
3. æ‰¾å‡ºå…±åŒå‘ç°å’Œåˆ†æ­§ç‚¹
4. æ¨èä¸´åºŠåº”ç”¨å»ºè®®

æ–‡çŒ®1: [æ ‡é¢˜å’Œæ‘˜è¦]
æ–‡çŒ®2: [æ ‡é¢˜å’Œæ‘˜è¦]
æ–‡çŒ®3: [æ ‡é¢˜å’Œæ‘˜è¦]
```

## æ–¹æ³•2: Pythonè‡ªåŠ¨åŒ–æ‰¹é‡æ€»ç»“

### éœ€æ±‚æè¿°

**å‘AIæé—®**:
```
æˆ‘éœ€è¦ä¸€ä¸ªPythonè„šæœ¬:

è¾“å…¥: PDFæ–‡ä»¶å¤¹
åŠŸèƒ½:
1. æå–æ¯ç¯‡PDFçš„æ–‡æœ¬(ä¸»è¦æ˜¯æ‘˜è¦å’Œç»“è®º)
2. è°ƒç”¨OpenAI APIæ€»ç»“è¦ç‚¹
3. ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š(Excelæˆ–Word)

è¦æ±‚:
- æ‰¹é‡å¤„ç†50ç¯‡æ–‡çŒ®
- æ€»ç»“æ ¼å¼ç»Ÿä¸€
- åŒ…å«è¯„åˆ†(ç›¸å…³æ€§1-5åˆ†)
```

### AIç”Ÿæˆä»£ç 

```python
"""
AIæ–‡çŒ®è‡ªåŠ¨æ€»ç»“å·¥å…·
"""

import pdfplumber
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import openai
import time

# é…ç½®OpenAI API
openai.api_key = "your-api-key-here"

def extract_abstract_from_pdf(pdf_path, max_pages=3):
    """
    ä»PDFæå–æ‘˜è¦å’Œå…³é”®éƒ¨åˆ†

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        max_pages: æœ€å¤šè¯»å–é¡µæ•°

    Returns:
        æå–çš„æ–‡æœ¬
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""

            # æå–å‰å‡ é¡µ(é€šå¸¸åŒ…å«æ‘˜è¦)
            for page in pdf.pages[:max_pages]:
                text += page.extract_text() + "\n"

            # å°è¯•å®šä½æ‘˜è¦éƒ¨åˆ†
            if "abstract" in text.lower():
                # ç®€å•æå–Abstractåˆ°Introductionä¹‹é—´çš„æ–‡æœ¬
                start = text.lower().find("abstract")
                end = text.lower().find("introduction", start)

                if start != -1:
                    if end != -1:
                        text = text[start:end]
                    else:
                        text = text[start:start+2000]  # é™åˆ¶é•¿åº¦

            return text[:4000]  # é™åˆ¶åœ¨4000å­—ç¬¦

    except Exception as e:
        print(f"âš ï¸  è¯»å–å¤±è´¥: {e}")
        return None

def summarize_with_ai(text, title=""):
    """
    ä½¿ç”¨OpenAI APIæ€»ç»“æ–‡çŒ®

    Args:
        text: æ–‡çŒ®æ–‡æœ¬
        title: æ–‡çŒ®æ ‡é¢˜

    Returns:
        æ€»ç»“å­—å…¸
    """
    prompt = f"""
è¯·æ€»ç»“ä»¥ä¸‹åŒ»å­¦æ–‡çŒ®,ä»¥JSONæ ¼å¼è¿”å›:

æ–‡çŒ®æ ‡é¢˜: {title}

æ–‡çŒ®å†…å®¹:
{text}

è¯·æå–:
1. objective: ç ”ç©¶ç›®çš„(1å¥è¯)
2. methods: ç ”ç©¶æ–¹æ³•(æ ·æœ¬é‡ã€è®¾è®¡)
3. results: ä¸»è¦å‘ç°(åŒ…å«å…³é”®æ•°æ®)
4. conclusion: ç»“è®º
5. limitations: å±€é™æ€§
6. relevance_score: ä¸´åºŠç›¸å…³æ€§è¯„åˆ†(1-5åˆ†)
7. key_points: 3ä¸ªæ ¸å¿ƒè¦ç‚¹(æ•°ç»„)

ä»¥JSONæ ¼å¼è¿”å›ã€‚
"""

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",  # æˆ– gpt-3.5-turbo
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ–‡çŒ®åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # é™ä½éšæœºæ€§
            max_tokens=800
        )

        summary = response.choices[0].message.content

        # è§£æJSON(ç®€åŒ–å¤„ç†)
        import json
        try:
            summary_dict = json.loads(summary)
        except:
            # å¦‚æœAIæ²¡æœ‰è¿”å›çº¯JSON,æ‰‹åŠ¨æå–
            summary_dict = {
                'objective': '',
                'methods': '',
                'results': '',
                'conclusion': summary[:200],
                'limitations': '',
                'relevance_score': 3,
                'key_points': []
            }

        return summary_dict

    except Exception as e:
        print(f"âš ï¸  AIæ€»ç»“å¤±è´¥: {e}")
        return None

def batch_summarize(pdf_folder, metadata_excel=None):
    """
    æ‰¹é‡æ€»ç»“æ–‡çŒ®

    Args:
        pdf_folder: PDFæ–‡ä»¶å¤¹
        metadata_excel: å¯é€‰,ä¹‹å‰ç”Ÿæˆçš„å…ƒæ•°æ®æ–‡ä»¶
    """
    print("=" * 60)
    print("AIæ–‡çŒ®è‡ªåŠ¨æ€»ç»“å·¥å…·")
    print("=" * 60)

    # è¯»å–PDFåˆ—è¡¨
    folder = Path(pdf_folder)
    pdf_files = list(folder.glob('*.pdf'))

    if not pdf_files:
        print("æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return

    print(f"\næ‰¾åˆ°{len(pdf_files)}ç¯‡æ–‡çŒ®")

    # å¦‚æœæœ‰å…ƒæ•°æ®,åŠ è½½æ ‡é¢˜ä¿¡æ¯
    titles_dict = {}
    if metadata_excel:
        df_meta = pd.read_excel(metadata_excel)
        titles_dict = dict(zip(df_meta['Filename'], df_meta['Title']))

    summaries = []

    for pdf_file in tqdm(pdf_files, desc="AIæ€»ç»“ä¸­"):
        filename = pdf_file.name
        title = titles_dict.get(filename, filename)

        # 1. æå–æ–‡æœ¬
        text = extract_abstract_from_pdf(pdf_file)

        if not text:
            continue

        # 2. AIæ€»ç»“
        summary = summarize_with_ai(text, title)

        if summary:
            summary['filename'] = filename
            summary['title'] = title
            summaries.append(summary)

        # é¿å…APIè¯·æ±‚è¿‡å¿«
        time.sleep(1)

    # ä¿å­˜ç»“æœ
    save_summaries(summaries)

    print(f"\nâœ“ æ€»ç»“å®Œæˆ! å…±{len(summaries)}ç¯‡")

def save_summaries(summaries, output_file='literature_summaries.xlsx'):
    """
    ä¿å­˜æ€»ç»“åˆ°Excel

    Args:
        summaries: æ€»ç»“åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶å
    """
    # è½¬æ¢ä¸ºDataFrame
    rows = []
    for s in summaries:
        row = {
            'Filename': s['filename'],
            'Title': s['title'],
            'Objective': s.get('objective', ''),
            'Methods': s.get('methods', ''),
            'Results': s.get('results', ''),
            'Conclusion': s.get('conclusion', ''),
            'Limitations': s.get('limitations', ''),
            'Relevance Score': s.get('relevance_score', 0),
            'Key Points': '; '.join(s.get('key_points', []))
        }
        rows.append(row)

    df = pd.DataFrame(rows)

    # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
    df = df.sort_values('Relevance Score', ascending=False)

    # ä¿å­˜
    df.to_excel(output_file, index=False, engine='openpyxl')

    print(f"âœ“ æ€»ç»“ä¿å­˜è‡³: {output_file}")

    # ç”ŸæˆWordæŠ¥å‘Š
    generate_word_report(summaries, 'literature_summaries.docx')

def generate_word_report(summaries, output_file):
    """ç”ŸæˆWordæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    from docx import Document
    from docx.shared import Pt, RGBColor

    doc = Document()

    # æ ‡é¢˜
    doc.add_heading('æ–‡çŒ®æ€»ç»“æŠ¥å‘Š', 0)

    # æŒ‰ç›¸å…³æ€§æ’åº
    summaries_sorted = sorted(
        summaries,
        key=lambda x: x.get('relevance_score', 0),
        reverse=True
    )

    for idx, summary in enumerate(summaries_sorted, 1):
        # æ–‡çŒ®æ ‡é¢˜
        heading = doc.add_heading(f"{idx}. {summary['title']}", 1)

        # ç›¸å…³æ€§è¯„åˆ†
        score = summary.get('relevance_score', 0)
        p = doc.add_paragraph()
        p.add_run(f"ç›¸å…³æ€§è¯„åˆ†: {'â­' * score} ({score}/5)").bold = True

        # ç ”ç©¶ç›®çš„
        doc.add_heading('ç ”ç©¶ç›®çš„', 2)
        doc.add_paragraph(summary.get('objective', 'N/A'))

        # ç ”ç©¶æ–¹æ³•
        doc.add_heading('ç ”ç©¶æ–¹æ³•', 2)
        doc.add_paragraph(summary.get('methods', 'N/A'))

        # ä¸»è¦å‘ç°
        doc.add_heading('ä¸»è¦å‘ç°', 2)
        doc.add_paragraph(summary.get('results', 'N/A'))

        # ç»“è®º
        doc.add_heading('ç»“è®º', 2)
        doc.add_paragraph(summary.get('conclusion', 'N/A'))

        # æ ¸å¿ƒè¦ç‚¹
        if summary.get('key_points'):
            doc.add_heading('æ ¸å¿ƒè¦ç‚¹', 2)
            for point in summary['key_points']:
                doc.add_paragraph(point, style='List Bullet')

        # å±€é™æ€§
        if summary.get('limitations'):
            doc.add_heading('å±€é™æ€§', 2)
            doc.add_paragraph(summary['limitations'])

        # åˆ†é¡µ
        if idx < len(summaries_sorted):
            doc.add_page_break()

    doc.save(output_file)
    print(f"âœ“ WordæŠ¥å‘Šä¿å­˜è‡³: {output_file}")

def main():
    """ä¸»ç¨‹åº"""
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python ai_summarize.py <PDFæ–‡ä»¶å¤¹> [å…ƒæ•°æ®Excel]")
        print("\nç¤ºä¾‹: python ai_summarize.py ./papers literature_metadata.xlsx")
        return

    pdf_folder = sys.argv[1]
    metadata_excel = sys.argv[2] if len(sys.argv) > 2 else None

    batch_summarize(pdf_folder, metadata_excel)

if __name__ == "__main__":
    main()
```

### ä½¿ç”¨æ–¹æ³•

#### 1. é…ç½®APIå¯†é’¥

```python
# æ–¹æ³•1: ç›´æ¥åœ¨ä»£ç ä¸­(ä¸æ¨è)
openai.api_key = "sk-..."

# æ–¹æ³•2: ç¯å¢ƒå˜é‡(æ¨è)
import os
openai.api_key = os.getenv("OPENAI_API_KEY")
```

#### 2. è¿è¡Œ

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="sk-..."

# è¿è¡Œè„šæœ¬
python ai_summarize.py ./papers literature_metadata.xlsx
```

**è¾“å‡º**:
```
============================================================
AIæ–‡çŒ®è‡ªåŠ¨æ€»ç»“å·¥å…·
============================================================

æ‰¾åˆ°50ç¯‡æ–‡çŒ®
AIæ€»ç»“ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:30<00:00]

âœ“ æ€»ç»“å®Œæˆ! å…±50ç¯‡
âœ“ æ€»ç»“ä¿å­˜è‡³: literature_summaries.xlsx
âœ“ WordæŠ¥å‘Šä¿å­˜è‡³: literature_summaries.docx
```

## æ–¹æ³•3: ä½¿ç”¨Claudeæˆ–å…¶ä»–AI

### å¯¹æ¯”ä¸åŒAIæ¨¡å‹

| æ¨¡å‹ | ä¼˜åŠ¿ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| GPT-4 | ç†è§£åŠ›å¼º,æ€»ç»“å‡†ç¡® | è¾ƒé«˜ | é‡è¦æ–‡çŒ®æ·±åº¦åˆ†æ |
| GPT-3.5-turbo | é€Ÿåº¦å¿«,æˆæœ¬ä½ | ä½ | æ‰¹é‡å¿«é€Ÿç­›é€‰ |
| Claude | é•¿æ–‡æœ¬å¤„ç†å¥½ | ä¸­ç­‰ | æ•´ç¯‡è®ºæ–‡æ€»ç»“ |
| æœ¬åœ°æ¨¡å‹ | å…è´¹,éšç§ | æ—  | å¤§é‡æ–‡çŒ®,æ•æ„Ÿæ•°æ® |

### Claude APIç¤ºä¾‹

```python
import anthropic

def summarize_with_claude(text, title=""):
    """ä½¿ç”¨Claudeæ€»ç»“"""
    client = anthropic.Anthropic(api_key="your-api-key")

    message = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"è¯·æ€»ç»“è¿™ç¯‡åŒ»å­¦æ–‡çŒ®:\n\n{title}\n\n{text}"
        }]
    )

    return message.content[0].text
```

## å®ç”¨æŠ€å·§

### æŠ€å·§1: è‡ªå®šä¹‰æ€»ç»“æ¨¡æ¿

```python
# é’ˆå¯¹ç³»ç»Ÿç»¼è¿°çš„æ€»ç»“æ¨¡æ¿
SYSTEMATIC_REVIEW_PROMPT = """
è¿™æ˜¯ä¸€ç¯‡ç³»ç»Ÿç»¼è¿°,è¯·æå–:
1. çº³å…¥ç ”ç©¶æ•°é‡å’Œè´¨é‡
2. Metaåˆ†æç»“æœ(å¦‚æœ‰)
3. å¼‚è´¨æ€§åˆ†æ
4. è¯æ®ç­‰çº§
5. ä¸´åºŠæ¨èå¼ºåº¦
"""

# é’ˆå¯¹RCTçš„æ€»ç»“æ¨¡æ¿
RCT_PROMPT = """
è¿™æ˜¯ä¸€ç¯‡éšæœºå¯¹ç…§è¯•éªŒ,è¯·æå–:
1. PICO (äººç¾¤ã€å¹²é¢„ã€å¯¹ç…§ã€ç»“å±€)
2. æ ·æœ¬é‡å’Œéšè®¿æ—¶é—´
3. ä¸»è¦ç»“å±€æŒ‡æ ‡å’Œæ¬¡è¦ç»“å±€
4. ä¸è‰¯äº‹ä»¶
5. NNT/NNH (éœ€è¦æ²»ç–—äººæ•°)
"""
```

### æŠ€å·§2: ç”Ÿæˆæ–‡çŒ®ç»¼è¿°

**Prompt**:
```
åŸºäºè¿™10ç¯‡æ–‡çŒ®çš„æ€»ç»“,å¸®æˆ‘å†™ä¸€æ®µ300å­—çš„æ–‡çŒ®ç»¼è¿°,åŒ…æ‹¬:
- å½“å‰ç ”ç©¶ç°çŠ¶
- ä¸»è¦å‘ç°å’Œäº‰è®®
- ç ”ç©¶ç©ºç™½

[ç²˜è´´10ç¯‡æ–‡çŒ®çš„AIæ€»ç»“]
```

### æŠ€å·§3: æ‰¹é‡ç¿»è¯‘

```python
def translate_summary(summary_text, target_lang='zh'):
    """å°†è‹±æ–‡æ€»ç»“ç¿»è¯‘ä¸ºä¸­æ–‡"""
    prompt = f"å°†ä»¥ä¸‹åŒ»å­¦æ–‡çŒ®æ€»ç»“ç¿»è¯‘ä¸º{target_lang}:\n\n{summary_text}"
    # è°ƒç”¨AI API...
```

## æˆæœ¬ä¼°ç®—

**ä½¿ç”¨GPT-4æ‰¹é‡æ€»ç»“**:
- 50ç¯‡æ–‡çŒ®
- æ¯ç¯‡çº¦3000 tokens (è¾“å…¥) + 500 tokens (è¾“å‡º)
- GPT-4æˆæœ¬: $0.03/1K input + $0.06/1K output
- æ€»æˆæœ¬: 50 Ã— (3Ã—0.03 + 0.5Ã—0.06) â‰ˆ $6

**ä½¿ç”¨GPT-3.5-turbo**:
- æ€»æˆæœ¬: 50 Ã— (3Ã—0.0015 + 0.5Ã—0.002) â‰ˆ $0.28

ğŸ’¡ **å»ºè®®**: å…ˆç”¨GPT-3.5å¿«é€Ÿç­›é€‰,é‡ç‚¹æ–‡çŒ®ç”¨GPT-4æ·±åº¦åˆ†æ

## æ£€æŸ¥æ¸…å•

- [ ] æ€»ç»“å‡†ç¡®åæ˜ æ–‡çŒ®å†…å®¹
- [ ] å…³é”®æ•°æ®è¢«æå–å‡ºæ¥
- [ ] ç›¸å…³æ€§è¯„åˆ†åˆç†
- [ ] æ ¼å¼ç»Ÿä¸€ä¾¿äºå¯¹æ¯”
- [ ] æ§åˆ¶äº†APIæˆæœ¬

## ä¸‹ä¸€æ­¥

[9.7 å®Œæ•´å·¥å…·:æ–‡çŒ®ç®¡ç†åŠ©æ‰‹](9.7-complete-tool.md) - æ•´åˆæ‰€æœ‰åŠŸèƒ½çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
