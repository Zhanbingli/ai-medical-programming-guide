# 9.6 AI总结文献要点

## 场景需求

**阅读文献的困境**:
```
下载了50篇文献,每篇10-20页
完整阅读需要: 50篇 × 30分钟 = 25小时 😫

关键问题:
- 哪些文献最相关?
- 每篇的核心发现是什么?
- 有哪些局限性?
```

**AI解决方案**:
```
AI快速阅读 → 3分钟/篇
提取核心要点 → 生成结构化摘要
时间节省: 25小时 → 2.5小时 (效率提升10倍!)
```

## 方法1: 使用ChatGPT总结

### 基础Prompt

**复制PDF文本,向ChatGPT提问**:
```
请总结这篇医学文献,包括:

1. 研究目的
2. 研究方法(样本量、设计类型)
3. 主要发现(用数据支持)
4. 结论和临床意义
5. 局限性

要求:
- 每部分3-5句话
- 突出关键数据
- 客观中立

[粘贴文献全文或摘要]
```

### 进阶Prompt:对比分析

```
我有3篇关于高血压治疗的文献,请帮我:

1. 对比研究设计和样本量
2. 对比主要结果和有效性
3. 找出共同发现和分歧点
4. 推荐临床应用建议

文献1: [标题和摘要]
文献2: [标题和摘要]
文献3: [标题和摘要]
```

## 方法2: Python自动化批量总结

### 需求描述

**向AI提问**:
```
我需要一个Python脚本:

输入: PDF文件夹
功能:
1. 提取每篇PDF的文本(主要是摘要和结论)
2. 调用OpenAI API总结要点
3. 生成结构化报告(Excel或Word)

要求:
- 批量处理50篇文献
- 总结格式统一
- 包含评分(相关性1-5分)
```

### AI生成代码

```python
"""
AI文献自动总结工具
"""

import pdfplumber
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import openai
import time

# 配置OpenAI API
openai.api_key = "your-api-key-here"

def extract_abstract_from_pdf(pdf_path, max_pages=3):
    """
    从PDF提取摘要和关键部分

    Args:
        pdf_path: PDF文件路径
        max_pages: 最多读取页数

    Returns:
        提取的文本
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""

            # 提取前几页(通常包含摘要)
            for page in pdf.pages[:max_pages]:
                text += page.extract_text() + "\n"

            # 尝试定位摘要部分
            if "abstract" in text.lower():
                # 简单提取Abstract到Introduction之间的文本
                start = text.lower().find("abstract")
                end = text.lower().find("introduction", start)

                if start != -1:
                    if end != -1:
                        text = text[start:end]
                    else:
                        text = text[start:start+2000]  # 限制长度

            return text[:4000]  # 限制在4000字符

    except Exception as e:
        print(f"⚠️  读取失败: {e}")
        return None

def summarize_with_ai(text, title=""):
    """
    使用OpenAI API总结文献

    Args:
        text: 文献文本
        title: 文献标题

    Returns:
        总结字典
    """
    prompt = f"""
请总结以下医学文献,以JSON格式返回:

文献标题: {title}

文献内容:
{text}

请提取:
1. objective: 研究目的(1句话)
2. methods: 研究方法(样本量、设计)
3. results: 主要发现(包含关键数据)
4. conclusion: 结论
5. limitations: 局限性
6. relevance_score: 临床相关性评分(1-5分)
7. key_points: 3个核心要点(数组)

以JSON格式返回。
"""

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",  # 或 gpt-3.5-turbo
            messages=[
                {"role": "system", "content": "你是一个医学文献分析专家。"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # 降低随机性
            max_tokens=800
        )

        summary = response.choices[0].message.content

        # 解析JSON(简化处理)
        import json
        try:
            summary_dict = json.loads(summary)
        except:
            # 如果AI没有返回纯JSON,手动提取
            summary_dict = {
                'objective': '',
                'methods': '',
                'results': '',
                'conclusion': summary[:200],
                'limitations': '',
                'relevance_score': 3,
                'key_points': []
            }

        return summary_dict

    except Exception as e:
        print(f"⚠️  AI总结失败: {e}")
        return None

def batch_summarize(pdf_folder, metadata_excel=None):
    """
    批量总结文献

    Args:
        pdf_folder: PDF文件夹
        metadata_excel: 可选,之前生成的元数据文件
    """
    print("=" * 60)
    print("AI文献自动总结工具")
    print("=" * 60)

    # 读取PDF列表
    folder = Path(pdf_folder)
    pdf_files = list(folder.glob('*.pdf'))

    if not pdf_files:
        print("未找到PDF文件")
        return

    print(f"\n找到{len(pdf_files)}篇文献")

    # 如果有元数据,加载标题信息
    titles_dict = {}
    if metadata_excel:
        df_meta = pd.read_excel(metadata_excel)
        titles_dict = dict(zip(df_meta['Filename'], df_meta['Title']))

    summaries = []

    for pdf_file in tqdm(pdf_files, desc="AI总结中"):
        filename = pdf_file.name
        title = titles_dict.get(filename, filename)

        # 1. 提取文本
        text = extract_abstract_from_pdf(pdf_file)

        if not text:
            continue

        # 2. AI总结
        summary = summarize_with_ai(text, title)

        if summary:
            summary['filename'] = filename
            summary['title'] = title
            summaries.append(summary)

        # 避免API请求过快
        time.sleep(1)

    # 保存结果
    save_summaries(summaries)

    print(f"\n✓ 总结完成! 共{len(summaries)}篇")

def save_summaries(summaries, output_file='literature_summaries.xlsx'):
    """
    保存总结到Excel

    Args:
        summaries: 总结列表
        output_file: 输出文件名
    """
    # 转换为DataFrame
    rows = []
    for s in summaries:
        row = {
            'Filename': s['filename'],
            'Title': s['title'],
            'Objective': s.get('objective', ''),
            'Methods': s.get('methods', ''),
            'Results': s.get('results', ''),
            'Conclusion': s.get('conclusion', ''),
            'Limitations': s.get('limitations', ''),
            'Relevance Score': s.get('relevance_score', 0),
            'Key Points': '; '.join(s.get('key_points', []))
        }
        rows.append(row)

    df = pd.DataFrame(rows)

    # 按相关性评分排序
    df = df.sort_values('Relevance Score', ascending=False)

    # 保存
    df.to_excel(output_file, index=False, engine='openpyxl')

    print(f"✓ 总结保存至: {output_file}")

    # 生成Word报告
    generate_word_report(summaries, 'literature_summaries.docx')

def generate_word_report(summaries, output_file):
    """生成Word格式的详细报告"""
    from docx import Document
    from docx.shared import Pt, RGBColor

    doc = Document()

    # 标题
    doc.add_heading('文献总结报告', 0)

    # 按相关性排序
    summaries_sorted = sorted(
        summaries,
        key=lambda x: x.get('relevance_score', 0),
        reverse=True
    )

    for idx, summary in enumerate(summaries_sorted, 1):
        # 文献标题
        heading = doc.add_heading(f"{idx}. {summary['title']}", 1)

        # 相关性评分
        score = summary.get('relevance_score', 0)
        p = doc.add_paragraph()
        p.add_run(f"相关性评分: {'⭐' * score} ({score}/5)").bold = True

        # 研究目的
        doc.add_heading('研究目的', 2)
        doc.add_paragraph(summary.get('objective', 'N/A'))

        # 研究方法
        doc.add_heading('研究方法', 2)
        doc.add_paragraph(summary.get('methods', 'N/A'))

        # 主要发现
        doc.add_heading('主要发现', 2)
        doc.add_paragraph(summary.get('results', 'N/A'))

        # 结论
        doc.add_heading('结论', 2)
        doc.add_paragraph(summary.get('conclusion', 'N/A'))

        # 核心要点
        if summary.get('key_points'):
            doc.add_heading('核心要点', 2)
            for point in summary['key_points']:
                doc.add_paragraph(point, style='List Bullet')

        # 局限性
        if summary.get('limitations'):
            doc.add_heading('局限性', 2)
            doc.add_paragraph(summary['limitations'])

        # 分页
        if idx < len(summaries_sorted):
            doc.add_page_break()

    doc.save(output_file)
    print(f"✓ Word报告保存至: {output_file}")

def main():
    """主程序"""
    import sys

    if len(sys.argv) < 2:
        print("用法: python ai_summarize.py <PDF文件夹> [元数据Excel]")
        print("\n示例: python ai_summarize.py ./papers literature_metadata.xlsx")
        return

    pdf_folder = sys.argv[1]
    metadata_excel = sys.argv[2] if len(sys.argv) > 2 else None

    batch_summarize(pdf_folder, metadata_excel)

if __name__ == "__main__":
    main()
```

### 使用方法

#### 1. 配置API密钥

```python
# 方法1: 直接在代码中(不推荐)
openai.api_key = "sk-..."

# 方法2: 环境变量(推荐)
import os
openai.api_key = os.getenv("OPENAI_API_KEY")
```

#### 2. 运行

```bash
# 设置环境变量
export OPENAI_API_KEY="sk-..."

# 运行脚本
python ai_summarize.py ./papers literature_metadata.xlsx
```

**输出**:
```
============================================================
AI文献自动总结工具
============================================================

找到50篇文献
AI总结中: 100%|██████████| 50/50 [02:30<00:00]

✓ 总结完成! 共50篇
✓ 总结保存至: literature_summaries.xlsx
✓ Word报告保存至: literature_summaries.docx
```

## 方法3: 使用Claude或其他AI

### 对比不同AI模型

| 模型 | 优势 | 成本 | 适用场景 |
|------|------|------|---------|
| GPT-4 | 理解力强,总结准确 | 较高 | 重要文献深度分析 |
| GPT-3.5-turbo | 速度快,成本低 | 低 | 批量快速筛选 |
| Claude | 长文本处理好 | 中等 | 整篇论文总结 |
| 本地模型 | 免费,隐私 | 无 | 大量文献,敏感数据 |

### Claude API示例

```python
import anthropic

def summarize_with_claude(text, title=""):
    """使用Claude总结"""
    client = anthropic.Anthropic(api_key="your-api-key")

    message = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"请总结这篇医学文献:\n\n{title}\n\n{text}"
        }]
    )

    return message.content[0].text
```

## 实用技巧

### 技巧1: 自定义总结模板

```python
# 针对系统综述的总结模板
SYSTEMATIC_REVIEW_PROMPT = """
这是一篇系统综述,请提取:
1. 纳入研究数量和质量
2. Meta分析结果(如有)
3. 异质性分析
4. 证据等级
5. 临床推荐强度
"""

# 针对RCT的总结模板
RCT_PROMPT = """
这是一篇随机对照试验,请提取:
1. PICO (人群、干预、对照、结局)
2. 样本量和随访时间
3. 主要结局指标和次要结局
4. 不良事件
5. NNT/NNH (需要治疗人数)
"""
```

### 技巧2: 生成文献综述

**Prompt**:
```
基于这10篇文献的总结,帮我写一段300字的文献综述,包括:
- 当前研究现状
- 主要发现和争议
- 研究空白

[粘贴10篇文献的AI总结]
```

### 技巧3: 批量翻译

```python
def translate_summary(summary_text, target_lang='zh'):
    """将英文总结翻译为中文"""
    prompt = f"将以下医学文献总结翻译为{target_lang}:\n\n{summary_text}"
    # 调用AI API...
```

## 成本估算

**使用GPT-4批量总结**:
- 50篇文献
- 每篇约3000 tokens (输入) + 500 tokens (输出)
- GPT-4成本: $0.03/1K input + $0.06/1K output
- 总成本: 50 × (3×0.03 + 0.5×0.06) ≈ $6

**使用GPT-3.5-turbo**:
- 总成本: 50 × (3×0.0015 + 0.5×0.002) ≈ $0.28

💡 **建议**: 先用GPT-3.5快速筛选,重点文献用GPT-4深度分析

## 检查清单

- [ ] 总结准确反映文献内容
- [ ] 关键数据被提取出来
- [ ] 相关性评分合理
- [ ] 格式统一便于对比
- [ ] 控制了API成本

## 下一步

[9.7 完整工具:文献管理助手](9.7-complete-tool.md) - 整合所有功能的完整解决方案
