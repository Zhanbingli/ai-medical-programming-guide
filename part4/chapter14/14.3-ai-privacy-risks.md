# 14.3 AI工具使用的隐私风险

## AI工具的数据处理方式

### 商业AI的运作机制

```
你的输入 → AI服务器 → 处理 → 返回结果
              ↓
         存储?训练?
```

**关键问题**:
- ❓ 你的对话会被保存吗?
- ❓ 会用于训练模型吗?
- ❓ 能完全删除吗?
- ❓ 有人能看到吗?

### 主流AI工具的隐私政策

| 工具 | 数据使用 | 删除政策 | 隐私风险 |
|------|---------|---------|---------|
| **ChatGPT** | 用于训练(可选择退出) | 可删除对话历史 | ⚠️ 中 |
| **Claude** | 不用于训练 | 可删除对话 | ⚠️ 低 |
| **GitHub Copilot** | 代码可能被使用 | - | ⚠️ 中 |
| **Cursor** | 代码可能被分析 | - | ⚠️ 中 |
| **本地部署AI** | 完全本地 | 自主控制 | ✅ 最低 |

## 真实风险案例

### 案例1: ChatGPT数据泄露

**事件**(2023年3月):
```
ChatGPT出现bug,用户可以看到其他用户的对话标题

影响:
- 部分用户的对话主题泄露
- 涉及敏感信息的标题暴露
```

**教训**:
```
✗ 不要在对话标题中包含患者信息
✗ 即使是标题也可能泄露
```

### 案例2: 三星员工泄露机密

**事件**(2023年4月):
```
三星员工使用ChatGPT:
- 上传源代码寻求优化建议
- 分享会议纪要总结内容
- 输入设备测试数据

后果:
- 商业机密泄露风险
- 三星禁止员工使用ChatGPT
```

**教训**:
```
✗ 不要上传敏感代码
✗ 不要分享内部文档
✗ 不要输入机密数据
```

### 案例3: 医疗AI助手泄露

**假设场景**:
```
医生: "患者张三,男,45岁,身份证110101197901011234,
      诊断为冠心病,既往史包括糖尿病10年..."

AI训练:
- 这些信息可能被用于训练
- 未来AI可能生成类似案例
- 患者隐私永久泄露
```

## 安全使用AI的原则

### 原则1: 永不上传真实患者信息

```python
# ❌ 错误示例
prompt = """
患者信息:
姓名: 张三
身份证: 110101199001011234
电话: 13800138000
诊断: 高血压、糖尿病
检查结果: 空腹血糖7.5 mmol/L

请帮我分析治疗方案。
"""

# ✅ 正确示例
prompt = """
假设病例:
患者: 45岁男性
诊断: 高血压、糖尿病
检查: 空腹血糖7.5 mmol/L

请提供治疗建议。
"""
```

### 原则2: 使用泛化的案例

```python
# ❌ 具体案例
"王xx医生,患者今天血压180/110,怎么办?"

# ✅ 泛化案例
"对于血压180/110的急症,标准处理流程是什么?"
```

### 原则3: 代码中不包含敏感信息

```python
# ❌ 代码中包含真实数据
df = pd.read_excel('patients_2024_01.xlsx')
# 文件名暴露时间和内容

patients = [
    {'name': '张三', 'id': '110101199001011234'},
    {'name': '李四', 'id': '110101199102021234'}
]
# 硬编码真实数据

# ✅ 使用示例数据
df = pd.read_excel('sample_data.xlsx')

patients = [
    {'name': 'Patient_A', 'id': 'ID_001'},
    {'name': 'Patient_B', 'id': 'ID_002'}
]
```

### 原则4: 优先使用本地工具

```
云端AI: ChatGPT, Claude
  ↓
本地AI: Ollama, LM Studio, GPT4All
  ↓
无AI: 传统编程
```

**本地AI部署**:
```bash
# 安装Ollama(本地运行大模型)
curl https://ollama.ai/install.sh | sh

# 下载模型
ollama pull llama2

# 使用
ollama run llama2
```

**优势**:
- ✅ 数据不离开本地
- ✅ 完全隐私保护
- ✅ 无网络依赖

**劣势**:
- ❌ 需要较好硬件(16GB+ RAM)
- ❌ 模型能力可能不如商业AI

## 不同场景的AI使用策略

### 场景1: 学习编程

**风险**: 低
**策略**: 可以自由使用商业AI

```python
# ✅ 安全
"Python如何读取Excel文件?"
"pandas的groupby函数怎么用?"
"这段代码为什么报错?"
```

### 场景2: 编写通用工具

**风险**: 低-中
**策略**: 使用通用示例数据

```python
# ✅ 安全
prompt = """
帮我写一个函数,输入DataFrame,输出:
1. 行数、列数
2. 缺失值统计
3. 数值列的基本统计

示例数据:
   age  weight  height
0   45    70.5    175
1   38    62.0    165
"""
```

### 场景3: 分析真实数据

**风险**: 高
**策略**:
1. 先脱敏数据
2. 或使用本地AI
3. 或只咨询方法论,不上传数据

```python
# ❌ 危险
"这是我的患者数据,帮我分析..." + 真实数据

# ✅ 安全方法1: 只问方法
"我有100例高血压患者的血压数据,如何进行统计分析?"

# ✅ 安全方法2: 使用模拟数据
"我有类似这样的数据(模拟):[示例数据],应该如何分析?"
```

### 场景4: 调试包含患者数据的代码

**风险**: 极高
**策略**: 绝不上传,或完全脱敏后上传

```python
# ❌ 极度危险
"这段代码报错,帮我看看:
df = pd.read_excel('patients.xlsx')
print(df.head())
# 输出包含真实患者数据"

# ✅ 安全方法
"这段代码报错,帮我看看:
df = pd.read_excel('data.xlsx')
print(df.head())
# 预期输出: age, gender, diagnosis等列
# 实际错误: KeyError: 'age'"
```

## AI工具隐私设置

### ChatGPT隐私设置

```
1. 关闭聊天历史
   Settings → Data Controls → Chat History & Training → Off

2. 删除历史对话
   Settings → Data Controls → Delete all chats

3. 申请导出数据
   Settings → Data Controls → Export data

4. 选择退出训练
   Settings → Data Controls → Improve the model for everyone → Off
```

### GitHub Copilot隐私设置

```
VSCode设置:
1. 禁止代码收集:
   Settings → Extensions → GitHub Copilot
   → Allow suggestions matching public code: Off

2. 禁止遥测:
   Settings → Telemetry → Off
```

## 企业/医院AI使用政策

### 政策模板

```markdown
## AI工具使用规范

### 禁止行为
❌ 上传患者信息到公共AI平台
❌ 使用AI处理敏感医疗数据
❌ 分享内部文档、代码到AI
❌ 在AI对话中透露机构信息

### 允许行为
✅ 学习编程知识
✅ 咨询通用技术问题
✅ 使用脱敏数据请求帮助
✅ 使用本地部署的AI工具

### 推荐做法
1. 优先使用本地AI
2. 使用前进行数据脱敏
3. 定期清除AI对话历史
4. 关闭AI训练数据收集

### 违规后果
- 初次: 警告
- 再次: 停止使用权限
- 严重: 纪律处分
```

## AI隐私风险自查清单

使用AI前,问自己:

- [ ] 这段对话包含真实患者信息吗?
- [ ] 这些信息如果泄露,会造成什么后果?
- [ ] 我能用脱敏或模拟数据替代吗?
- [ ] 我知道这个AI工具的隐私政策吗?
- [ ] 我关闭了数据收集和训练功能吗?
- [ ] 如果有疑虑,我能使用本地AI吗?
- [ ] 我遵守了医院/机构的AI使用规定吗?

**如果有任何疑虑,不要使用!**

## 未来趋势

### 1. 本地化AI

```
趋势: 越来越多的医疗机构部署本地AI
- 医院内网部署大模型
- 数据不出医院
- 完全隐私保护
```

### 2. 隐私增强技术

```
- 联邦学习: 模型训练不共享原始数据
- 差分隐私: 添加噪声保护隐私
- 同态加密: 加密状态下计算
```

### 3. 监管趋严

```
- AI使用审计要求
- 数据泄露重罚
- 强制隐私影响评估
```

## 总结

**黄金法则**:
```
如果你不愿意在微信群里公开发这段话,
就不要发给任何商业AI!
```

**记住**:
1. 患者隐私 > 便利性
2. 有疑虑时,不使用
3. 能本地就本地
4. 永远假设会被泄露

## 下一步

[14.4 安全存储与传输](14.4-secure-storage.md) - 数据安全最佳实践
